---
title: "Human Behaviour R project"
author: "Lukasz K Bonenberg"
output:
  html_document:
    highlight: pygments
    theme: cerulean
    #toc: yes
---

```{r, echo=FALSE,warning = FALSE}
setwd("d:/tmp/Dropbox/Edu/other/BigDataMeasuringandPredictingHumanBehaviour/")
rm(list=ls(all=TRUE))

library(knitr)
opts_chunk$set(echo = T, cache = T, cache.path = "cache/", fig.path = "figure/", warning = FALSE,error=FALSE,message=F,strip.white=T)
#http://yihui.name/knitr/options/
```


#<a name="intro">Introdcution</a> 

In this project we will try and visualise what people are looking for at Wikipedia using < http://stats.grok.se/> website. 


# downlad data

```{r}
#install.packages("RCurl")  # Install the JSON parser
library(RCurl)
library(RJSONIO)  # Load the JSON parser

targetURL<-"http://stats.grok.se/json/en/201601/Friday"
rawData <- getURL(targetURL)
parsedData <- fromJSON(rawData)

```

## understand data

Curently dates are treated as our row names. Lets create data frame and sort out dates and data order.

```{r}
names(parsedData$daily_views)

parseData <- data.frame(Date=names(parsedData$daily_views),  # get the names
        Views=parsedData$daily_views,  # get the data points
        row.names=NULL) # stop using the dates as names
parseData$Dates<-as.Date(parseData$Date)
parseData$Date<-NULL #error when doing directly
parseData <- parseData[order(parseData$Dates),]
row.names(parseData) <- NULL #reset row numbers

```


## Visualise data

```{r}
library(ggplot2)
library(gtable)

#plot(parsedData$daily_views)
plot_data <- qplot(x=Dates,y=Views,data=parseData,group=1)
plot_data +geom_line()+theme_bw()+theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


# Automating the process

Let's now lookinto automating this process. Overall principle is quite easy, yust acccess relevant website address: stats.grok.se/json/en/yyyymm/KEYword 

```{r}
#month <- 1
#year <- 2016
keyword<-"smoke"
```

We want to list all data 2008-October 2014

```{r}

baseUrl <- "stats.grok.se/json/en/"
listUrl <- NULL #init

for (year in (2008:2014)){
  for (month in 1:12){
    if ((year == 2014) && (month>10)){
      next #skip run
    }
    date <- sprintf("%04i%02i",year,month)
    getUrl<- paste0(baseUrl,date,'/',keyword)
    
    #print(getUrl)
    listUrl <- c(listUrl,getUrl)
  
  }#month in 1:12){
}#year in (2008:2015){}


#print(listUrl)
```

Now we want to downlad it

```{r}
allViewsData <- NULL
for (theURL in listUrl) {
    
        cat("Downloading data from", theURL, "\n") 
        
        rawData <- getURL(theURL)
        parsedData <- fromJSON(rawData)
        viewsData <- data.frame(Date=names(parsedData$daily_views),
            Views=parsedData$daily_views,row.names=NULL)
                    
        allViewsData<-rbind(allViewsData,viewsData)
}

allViewsData$Date<-as.Date(allViewsData$Date) #convert to date
allViewsData<- allViewsData[order(allViewsData$Date),]
allViewsData<-subset(allViewsData,!is.na(Date))#remove nans


```

And visualise it


```{r}
library(ggplot2)
library(gtable)

#plot(allViewsData$daily_views)
plot_data <- qplot(x=Date,y=Views,data=allViewsData,group=1)
plot_data +geom_line()+theme_bw()+theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


## How much UK users google for the future

```{r}

#skip start and end of file - aditional notes
googleTrendsData<-read.csv("week6_googleTrends.csv",skip=4,header = T,nrow=582-5)

#sort out dates
googleTrendsData$Year<-as.numeric(substring(googleTrendsData$Week,first = 1,last = 4))

head(googleTrendsData)

```

Lets calculate index for 2012

```{r}
y2012trend <- subset(googleTrendsData,Year==2012)
futureIndex <- sum(y2012trend$X2013)/sum(y2012trend$X2011)
str4display<-sprintf("Future orientation index for UK in 2012 is >1, it is %.2f",futureIndex)
str4display
```


## How much ppl in the world google for the future


Lets first get all the files

```{r}
unzip("Trends.zip",list = F) #unzip it
allFileNames <- list.files("Trends")  #get file names

```


And then get data

```{r}
allFOI<-NULL

for (datafile in allFileNames){
  
  countryID<-substring(datafile, first=1, last=2) #get country codes from file name
  datafile<- paste0("Trends/",datafile) #add dir
  trendsData <- read.csv(datafile,skip=4,nrows = 476-5)
  trendsData$Year <- as.numeric(substring(trendsData$Week,first = 1, last = 4))
  
  
  trendsData2012 <- subset(trendsData, Year == 2012)
  FOI <- data.frame(Country= countryID, FOI = sum(trendsData2012$X2013)/ sum(trendsData2012$X2011))
  repoStr<- sprintf("Future index for %s in 2012 was %.2f",countryID,FOI[2])
  print(repoStr)
  
  allFOI<-rbind(allFOI,FOI)
}

unlink("Trends/*") #remove all files
```


## Do richer countries Google for more information about the future?

Lets dwell on the quiestion further and used additional data from [CIA World Factbook ](https://www.cia.gov/library/publications/the-world-factbook/)


```{r}
gdpPerCap<-read.csv("rawdata_2004.txt",sep = "\t",header = F,row.names = 1)
names(gdpPerCap)<-c("Country","GDP")

gdpPerCap$GDP<-substring(gdpPerCap$GDP,first = 2) #remove all $
gdpPerCap$GDP<-as.numeric(sub(pattern = ",", replacement = "",x=gdpPerCap$GDP)) #remove all commas

str(gdpPerCap)
```

Lets now change country name into country code

```{r}
#install.packages("countrycode")
library("countrycode")

gdpPerCap$Country <- countrycode(gdpPerCap$Country,origin="country.name", destination="iso2c") 
head(gdpPerCap)

FOI<-merge(allFOI,gdpPerCap,all.x= T) #inner join
head(FOI)

```


Lets visualise things

```{r}
library(ggplot2)

plot_data <- qplot(x=FOI,y=GDP,data=FOI)
plot_data +geom_point(shape=1) +theme_bw() + geom_smooth(method=lm,se=T)  #Add linear regression line

#geom_line()+theme_bw()+theme(axis.text.x = element_text(angle = 90, hjust = 1))

```




# Things to look up

* there is dedicated [gtrend package](https://github.com/PMassicotte/gtrendsR)

